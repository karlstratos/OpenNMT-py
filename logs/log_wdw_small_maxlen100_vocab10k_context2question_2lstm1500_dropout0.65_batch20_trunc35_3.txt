[2018-12-04 14:20:31,546 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:20:31,553 INFO]  * vocabulary size. source = 10005; target = 10005
[2018-12-04 14:20:31,554 INFO] Building model...
[2018-12-04 14:20:31,554 INFO] Building encoder......
[2018-12-04 14:20:31,699 INFO] RNNEncoder: type LSTM, bidir 0, layers 2, hidden size 1500, dropout 0.65
[2018-12-04 14:20:32,020 INFO] Building decoder......
[2018-12-04 14:20:32,166 INFO] ** Sharing word embedding matrix between src/tgt
[2018-12-04 14:20:32,166 INFO] InputFeedRNNDecoder: type LSTM, bidir 0, layers 2, hidden size 1500, general global attn (softmax), coverage attn 0, copy attn 0, dropout 0.65
[2018-12-04 14:20:32,626 INFO] Building NMTModel......
[2018-12-04 14:20:32,626 INFO] Building generator......
[2018-12-04 14:20:32,758 INFO] ** Sharing generator softmax with tgt word embedding
[2018-12-04 14:20:32,758 INFO] Initializing parameters......
[2018-12-04 14:20:32,758 INFO] WARNING: NOT USING XAVIER INITIALIZATION? WILL JUST USE UNIF(+- 0.10)
[2018-12-04 14:20:36,301 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(10005, 1500, padding_idx=1)
        )
      )
    )
    (rnn): LSTM(1500, 1500, num_layers=2, dropout=0.65)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(10005, 1500, padding_idx=1)
        )
      )
    )
    (dropout): Dropout(p=0.65)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.65)
      (layers): ModuleList(
        (0): LSTMCell(3000, 1500)
        (1): LSTMCell(1500, 1500)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=1500, out_features=1500, bias=False)
      (linear_out): Linear(in_features=3000, out_features=1500, bias=False)
    )
  )
  (generator): Sequential(
    (0): Linear(in_features=1500, out_features=10005, bias=True)
    (1): LogSoftmax()
  )
)
[2018-12-04 14:20:36,302 INFO] encoder: 51031500
[2018-12-04 14:20:36,302 INFO] decoder: 51784005
[2018-12-04 14:20:36,302 INFO] * number of parameters: 102815505
[2018-12-04 14:20:36,303 INFO] * Opt: sgd (rate 1.00000, maxgnorm 5.0, standard decay, decay_rate 0.5, start_decay_at 50000, decay_every 10000, ab1 0.90000, ab2 0.99900, adagradaccum 0.0, warmupsteps 4000, hiddensize 1500)
[2018-12-04 14:20:36,303 INFO] * model_saver built, using it to build trainer with 
[2018-12-04 14:20:36,303 INFO] Building trainer.......
[2018-12-04 14:20:36,303 INFO] * cross-entropy loss over 10005 tgt words (label smoothing 0.0)
[2018-12-04 14:20:36,304 INFO] Starting training on GPU: [1]
[2018-12-04 14:20:36,304 INFO] Start training...
[2018-12-04 14:20:38,190 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:20:43,409 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:20:48,725 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:20:53,983 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:20:59,165 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:04,324 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:09,528 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:14,757 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:19,927 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:25,081 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:30,297 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:35,513 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:40,716 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:45,935 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:51,152 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
[2018-12-04 14:21:56,454 INFO] Loading train dataset from data/wdw_small_maxlen100_vocab10k_context2question.train.0.pt, number of examples: 50000
